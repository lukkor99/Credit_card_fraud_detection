# Credit Card Fraud Detection | Logistic Regression 

## Project scope
<p align="justify">
  This project compares performance of classification algorithms, starting from simple logistic regression and ending on more complex like gradient boosted random forests. Whole process of data analysis and preprocessing led to competetive results. Validation process has shown that tree-based classificators can reach better performance than multiple logistic regression. Differences between random forest, LightGBM and XGBoost are not significant, but random forest classifier provided best results with optimal utilization of compute resources. <p>
    
1. fraud transactions
2. non-fraud transactions

<p align="justify"> Techniques and tools used to improve models performance:
<p>
    
  * undersampling and oversampling
  * feature selection
  * standardization
  * ROC and AUC
  * hyperparameters tuning
  
  
  
  
## Motivation
  
<p align="justify"> Project created for training. I've learned how to deal with unbalanced datasets and how to improve binary classifiers performance. I've also explored the meaning of particular metrics used in machine learning. <p>
  
## Used packages
  
  *   Pandas
  *   Numpy
  *   Seaborn
  *   Matplotlib
  *   Scikit Learn
  *   LightGNB
  *   XGBoost
  *   Joblib
  
## Links

  * [Image source](https://www.dignited.com/wp-content/uploads/2018/10/Credit-Card-Fraud-768x512.jpg)
  * [Datasets source](https://www.kaggle.com/mlg-ulb/creditcardfraud)
  * [Oversampling and Undersampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)
  * [Understanding AUC - ROC Curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)
  * [Interpretation of coefficients in logistic regression](https://www.polyu.edu.hk/cbs/sjpolit/logisticregression.html)
  
